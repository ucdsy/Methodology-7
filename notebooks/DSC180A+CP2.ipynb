{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "parent_dir=os.path.split(os.getcwd())[0]\n",
    "DBLP=parent_dir+'\\\\src\\\\data\\\\EN\\\\DBLP.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(DBLP):\n",
    "    # Read from text file\n",
    "    with open(DBLP, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "    token_count = 0\n",
    "    sentence_count = 0\n",
    "    for line in lines:\n",
    "        # We ignore the last char, because it is newline\n",
    "        token_count += len(line.decode()[:-1].replace('.', ' ').replace(',',' ').split())\n",
    "        # We ignore the last two chars, because it is newline and maybe a dot.\n",
    "        sentence_count += len(line.decode()[:-2].split('.'))\n",
    "    print('The number of documents: %s  The number of sentences: %s  The number of words: %s' % (len(lines), sentence_count, token_count))\n",
    "    word_list = []\n",
    "    for line in lines:\n",
    "        for word in line.decode()[:-1].replace('.', ' ').replace(',',' ').split():\n",
    "            word_list.append(word)\n",
    "    print('The number of unique words: '+len(set(word_list)))\n",
    "    document_length = []\n",
    "    for line in lines:\n",
    "        document_length.append(len(line.decode()[:-1].replace('.', '').replace(',','').split()))\n",
    "    filter_length = []\n",
    "    for l in document_length:\n",
    "        if l < 400:\n",
    "            filter_length.append(l)\n",
    "    plt.hist(filter_length, bins=int(200/2))\n",
    "    plt.savefig('dochist.png')\n",
    "    plt.show()\n",
    "    print(\"The number of outliers document: %i\" % (len(length) - len(filter_length)))\n",
    "    sentence_length = []\n",
    "    for line in lines:\n",
    "        for sentence in line.decode()[:-1].replace(',', ' ').split('.'):\n",
    "            if len(sentence) != 0:\n",
    "                sentence_length.append(len(sentence))\n",
    "    filter_sentence_length = []\n",
    "    for l in sentence_length:\n",
    "        if l < 400:\n",
    "            filter_sentence_length.append(l)\n",
    "    plt.hist(sentence_length, bins=int(200/2))\n",
    "    plt.savefig('sentencehist.png')\n",
    "    plt.show()\n",
    "    print(\"The number of outliers sentence: %i\" % (len(sentence_length) - len(filter_sentence_length)))\n",
    "    from collections import Counter\n",
    "    c = Counter(list(set(word_list)))\n",
    "    word_count = list(dict(c).values())\n",
    "    filter_word_count = []\n",
    "    for l in word_count:\n",
    "        if l < 50:\n",
    "            filter_word_count.append(l)\n",
    "\n",
    "    plt.hist(filter_word_count, bins=int(50/2))\n",
    "    plt.savefig('wordhist.png')\n",
    "    plt.show()\n",
    "    rare = np.count_nonzero(np.array(word_count) < 5)\n",
    "\n",
    "    print(\"The number of rare tokens is: %i\" % rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of documents: 5547032  The number of sentences: 8568673  The number of words: 91883474\n"
     ]
    }
   ],
   "source": [
    "token_count = 0\n",
    "sentence_count = 0\n",
    "for line in lines:\n",
    "    # We ignore the last char, because it is newline\n",
    "    token_count += len(line.decode()[:-1].replace('.', ' ').replace(',',' ').split())\n",
    "    # We ignore the last two chars, because it is newline and maybe a dot.\n",
    "    sentence_count += len(line.decode()[:-2].split('.'))\n",
    "    \n",
    "print('The number of documents: %s  The number of sentences: %s  The number of words: %s' % (len(lines), sentence_count, token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 91.0 GiB for an array with shape (91883474,) and data type <U266",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-42554c9b17de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mword_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The number of unique words: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 91.0 GiB for an array with shape (91883474,) and data type <U266"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "for line in lines:\n",
    "    for word in line.decode()[:-1].replace('.', ' ').replace(',',' ').split():\n",
    "        word_list.append(word)\n",
    "        \n",
    "print('The number of unique words: ' + str(len(np.unique(np.array(word_list)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_length = []\n",
    "for line in lines:\n",
    "    document_length.append(len(line.decode()[:-1].replace('.', '').replace(',','').split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length = []\n",
    "for l in document_length:\n",
    "    if l < 400:\n",
    "        filter_length.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(filter_length)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(filter_length, bins=int(200/2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of outliers document: %i\" % (len(length) - len(filter_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_length = []\n",
    "for line in lines:\n",
    "    for sentence in line.decode()[:-1].replace(',', ' ').split('.'):\n",
    "        if len(sentence) != 0:\n",
    "            sentence_length.append(len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_sentence_length = []\n",
    "for l in sentence_length:\n",
    "    if l < 400:\n",
    "        filter_sentence_length.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sentence_length, bins=int(200/2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of outliers sentence: %i\" % (len(sentence_length) - len(filter_sentence_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = list(dict(c).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_word_count = []\n",
    "for l in word_count:\n",
    "    if l < 50:\n",
    "        filter_word_count.append(l)\n",
    "\n",
    "plt.hist(filter_word_count, bins=int(50/2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = np.count_nonzero(np.array(word_count) < 5)\n",
    "\n",
    "print(\"The number of rare tokens is: %i\" % rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}